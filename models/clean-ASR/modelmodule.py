"""
ğŸ–¤ğŸ° JaeEun Baik, 2025
"""
import re
import random 
import logging
import jiwer
import torch
import wandb
import numpy as np
import pytorch_lightning as pl
from typing import List

import sentencepiece as spm

from util.utils_text import TokenProcessor, ErrorCalculator, preprocess_text
from modules.e2e_asr_model import e2eASR

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class ModelModule(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.model_config = config.model.asr
        self.kd_config = config.model.distillation
        self.use_kd = self.kd_config.using_distillation
        # self.kd_loss = KDLoss(model_config.distillation)
        # if self.use_kd:
        #     self.teacher_model = e2eASR(model_config.teacher)
        #     self.student_model = e2eASR(model_config.student)
            
        #     self.model = KDWrapper(self.teacher_model, self.student_model, self.kd_loss, model_config.distillation.target)
        # else:
        self.optim_config = config.optimizer
        self.lr = np.float64(self.optim_config.op_lr)
        self.tokenizer_path = config.data.tokenizer
        self.token_processor = TokenProcessor(config.data.tokenizer)
        self.trainer_config = config.trainer
        self.model = e2eASR(self.model_config, self.tokenizer_path)
        
        self.sp_processor = spm.SentencePieceProcessor()
        self.sp_processor.load(self.tokenizer_path)
        
        self.char_list = [self.sp_processor.id_to_piece(i) for i in range(self.model_config.decoder.odim)]
        self.error_calculator = ErrorCalculator(
            char_list=self.char_list,
            sym_space=self.model_config.sym_space,
            sym_blank=self.model_config.sym_blank,
            report_cer=self.model_config.report_cer,
            report_wer=self.model_config.report_wer
        )
      

    
    def training_step(self, batch, batch_idx):
        x, x_len, y, y_len = batch
        if self.use_kd:
            loss_dict = self.model(x, x_len, y, y_len)
            self.log("train/total_loss", loss_dict["total_loss"])
            self.log("train/kd_loss", loss_dict["kd_loss"])
            self.log("train/asr_loss", loss_dict["student_loss"])
            return loss_dict["total_loss"]
        else:
            loss = self.model(x, x_len, y, y_len)
            log_items = {
                "train/total_loss": loss.get("loss"),
                "train/cer": loss.get("cer"),
                "train/wer": loss.get("wer"),
            }
            
            # Noneì´ ì•„ë‹Œ ê°’ë§Œ logë¡œ ë„˜ê¹€
            for key, value in log_items.items():
                if value is not None:
                    self.log(key, value, prog_bar=True, sync_dist=True)
            
            return loss.get("loss")
    
    def validation_step(self, batch, batch_idx):
        x, x_len, y, y_len = batch 
        
        self.model.eval()

        if self.use_kd:
            loss_dict = self.model(x, x_len, y, y_len)
            self.log("val/loss", loss_dict["total_loss"])
            self.log("val/kd_loss", loss_dict["kd_loss"])
            self.log("val/asr_loss", loss_dict["student_loss"])
        else:
            loss_output = self.model(x, x_len, y, y_len) 
            log_items = {
                "val/loss": loss_output.get("loss"),
                "val/ctc_loss": loss_output.get("loss_ctc"),
                "val/att_loss": loss_output.get("loss_att"),
            }

            for key, value in log_items.items():
                if value is not None:
                    self.log(key, value, prog_bar=True, sync_dist=True)
            
            with torch.no_grad():
                recog_args = self.model_config.decoder 
                decoded_raw_output = self.model.recognize(x, x_len, y, y_len, recog_args=recog_args)
            
            predicted_transcriptions: List[str] = []

            # decoded_raw_outputì˜ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
            if self.model.decoder_type == 'ctc':
                # CTC decoder (greedy/beamsearch)ëŠ” List[str]ì„ ë°˜í™˜í•¨
                if isinstance(decoded_raw_output, list) and all(isinstance(d, str) for d in decoded_raw_output):
                    predicted_transcriptions = decoded_raw_output
                else:
                    logging.warning(f"CTC decoded output is not List[str]. Type: {type(decoded_raw_output)}")
                    predicted_transcriptions = [""] * len(x)

            elif self.model.decoder_type == 'rnnt':
                # RNN-T decoderëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¥¼ ë°˜í™˜í•¨ (ì˜ˆ: {'greedy_search': [['word1', 'word2'], ['word3']]})
                if isinstance(decoded_raw_output, dict) and decoded_raw_output:
                    # ë”•ì…”ë„ˆë¦¬ì˜ ì²« ë²ˆì§¸ ê°’ì„ ê°€ì ¸ì™€ì„œ List[List[str]] í˜•íƒœì¸ì§€ í™•ì¸
                    first_key_value = next(iter(decoded_raw_output.values()))
                    if isinstance(first_key_value, list) and all(isinstance(s, list) and all(isinstance(w, str) for w in s) for s in first_key_value):
                        # ê° ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸(ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸)ë¥¼ ê³µë°±ìœ¼ë¡œ ì¡°ì¸í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
                        predicted_transcriptions = [" ".join(word_list) for word_list in first_key_value]
                    else:
                        logging.warning(f"RNN-T decoded output dict value is not List[List[str]]. Actual type: {type(first_key_value)}")
                        predicted_transcriptions = [""] * len(x)
                else:
                    logging.warning("RNN-T decoded output is an empty dictionary or not a dict.")
                    predicted_transcriptions = [""] * len(x)

            elif self.model.decoder_type == 'transformer':
                # Transformer decoderëŠ” List[str]ì„ ë°˜í™˜í•¨ (greedy/beamsearch)
                if isinstance(decoded_raw_output, list) and all(isinstance(d, str) for d in decoded_raw_output):
                    predicted_transcriptions = decoded_raw_output
                else:
                    logging.warning(f"Transformer decoded output is not List[str]. Type: {type(decoded_raw_output)}")
                    predicted_transcriptions = [""] * len(x)
            
            else:
                logging.warning(f"Unsupported decoder type: {self.model.decoder_type}. Decoded output type: {type(decoded_raw_output)}")
                predicted_transcriptions = [""] * len(x)
            
            
            # # Ground Truth í…ìŠ¤íŠ¸ ë³€í™˜
            reference_transcriptions: List[str] = []
            for i in range(len(y)):
                gt_tokens = y[i].tolist()
                gt_text = self.token_processor.id2text(gt_tokens, filter_blank=True)
                reference_transcriptions.append(gt_text)
            
            # # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (WER ê³„ì‚°ì„ ìœ„í•´ ê³µí†µì ìœ¼ë¡œ ìˆ˜í–‰)
            try:
                processed_predicted_transcriptions = [preprocess_text(text) for text in predicted_transcriptions]
                processed_reference_transcriptions = [preprocess_text(text) for text in reference_transcriptions]
            except NameError:
                logging.warning("preprocess_text function not found. Using raw transcriptions for WER.")
                processed_predicted_transcriptions = predicted_transcriptions
                processed_reference_transcriptions = reference_transcriptions

            batch_wers = []
            try:
                import jiwer # jiwer ì„í¬íŠ¸ í™•ì¸
                for gt, pred in zip(processed_reference_transcriptions, processed_predicted_transcriptions):
                    if gt or pred: 
                        wer = jiwer.wer(gt, pred)
                        batch_wers.append(wer)
                    else:
                        logging.debug(f"Skipping WER for empty GT/PR. GT='{gt}', PR='{pred}'")
            except ImportError:
                logging.error("jiwer library not installed. Cannot calculate WER. Please install it with 'pip install jiwer'.")
                batch_wers = []

            if batch_wers:
                avg_wer = sum(batch_wers) / len(batch_wers)
                self.log("val_wer", avg_wer, prog_bar=True, sync_dist=True)
            else:
                self.log("val_wer", 1.0, prog_bar=True, sync_dist=True) 

            if batch_idx == 0 and len(processed_reference_transcriptions) > 0:
                logging.info(f"\n===== Batch {batch_idx}, Sample 0 (Decoder Type: {self.model.decoder_type}) =====")
                logging.info(f"GT: '{processed_reference_transcriptions[0]}'")
                logging.info(f"PR: '{processed_predicted_transcriptions[0]}'")
                if batch_wers:
                    logging.info(f"WER: {batch_wers[0]:.4f} ({batch_wers[0] * 100:.2f}%)")
                else:
                    logging.info("WER calculation skipped (no valid texts or jiwer not installed).")

                    
                
                
    def on_validation_epoch_start(self):
        """ì—í­ ì‹œì‘ ì‹œ WER í†µê³„ ì´ˆê¸°í™”"""
        self.val_wer_samples = []
        self.val_wer_sum = 0
        self.val_wer_count = 0

    def on_validation_epoch_end(self):
        """ì—í­ ì¢…ë£Œ ì‹œ WER í†µê³„ ì²˜ë¦¬ ë° ë¡œê¹…"""
        if self.val_wer_count > 0:
            avg_wer = self.val_wer_sum / self.val_wer_count
            self.log("val_wer", avg_wer)
            
            # íˆìŠ¤í† ê·¸ë¨ ë¡œê¹…
            if self.logger and hasattr(self.logger, "experiment"):
                import numpy as np
                import wandb
                if self.val_wer_samples:
                    self.logger.experiment.log({
                        "val_wer_histogram": wandb.Histogram(np.array(self.val_wer_samples)),
                        "global_step": self.global_step
                    })
            
            print(f"\n===== ê²€ì¦ ì™„ë£Œ =====")
            print(f"ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {self.val_wer_count}")
            print(f"í‰ê·  WER: {avg_wer:.4f} ({avg_wer*100:.2f}%)")    
                
                
    def test_step(self, batch, batch_idx):
        x, x_len, y, y_len = batch
        if self.use_kd:
            logits = self.student_model.encode(x, x_len)
            decoded_trans = self.student_model.recognize(logits)
        else:
            decoded_trans = self.model.recognize(x, x_len, y, y_len, self.model_config.decoder)

        ref_trans: List[str] = []
        for single_y_tokens in y:
            # ignore_id (padding)ë¥¼ ì œì™¸í•˜ê³  ì‹¤ì œ í† í°ë§Œ ì‚¬ìš©
            # self.model.sos, self.model.eos ë„ ê³ ë ¤í•˜ì—¬ ì œê±°í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì´ ë¶€ë¶„ì€ í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ êµ¬ì„±ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            filtered_tokens = [
                token.item() for token in single_y_tokens 
                if token.item() != self.model.ignore_id and 
                   token.item() != self.model.sos and 
                   token.item() != self.model.eos
            ]
            ref_trans.append(self.token_processor.id2text(filtered_tokens))
        
        cer_batch = self.error_calculator.calculate_cer(decoded_trans, ref_trans)
        wer_batch = self.error_calculator.calculate_wer(decoded_trans, ref_trans)
        
        # í‰ê·  CER/WER ë¡œê¹… (ë°°ì¹˜ ë‹¨ìœ„)
        self.log("test_cer", cer_batch, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log("test_wer", wer_batch, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        # í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ì ì¸ ë¡œê¹… ë˜ëŠ” ê²°ê³¼ ë°˜í™˜
        return {
            "decoded_transcriptions": decoded_trans,
            "reference_transcriptions": ref_trans,
            "cer_batch": cer_batch,
            "wer_batch": wer_batch
        }
        
            
    def configure_optimizers(self):
        # optimizer
        # Conformer ë…¼ë¬¸ì€ Adamì„ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ, Adamì— ë² íƒ€ì™€ ì—¡ì‹¤ë¡  ê°’ì„ ëª…ì‹œì ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        # LambdaLR ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© ì‹œ, ì˜µí‹°ë§ˆì´ì €ì˜ ì´ˆê¸° lrì€ 1.0ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
        initial_lr_for_scheduler = 1.0 

        if self.optim_config.type == "AdamW":
            optimizer = torch.optim.AdamW(self.parameters(), lr=initial_lr_for_scheduler, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-6)
        elif self.optim_config.type == "Adam":
            optimizer = torch.optim.Adam(self.parameters(), lr=initial_lr_for_scheduler, betas=(0.9, 0.98), eps=1e-9)
            
        # scheduler
        def transformer_lr_schedule(step, d_model, warmup_steps):
            """
            Transformer Learning Rate Schedule.
            ë…¼ë¬¸: peak learning rate 0.05 / sqrt(d)
            """
            if step == 0:
                step = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
            
            # Conformer ë…¼ë¬¸ì— ëª…ì‹œëœ ìµœê³  í•™ìŠµë¥  (0.05 / sqrt(d))ì„ ì •í™•íˆ ë°˜ì˜
            # '0.05' ìƒìˆ˜ë¥¼ ê³±í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
            # d_model ** -0.5 == 1 / sqrt(d_model)
            peak_lr_scale_factor = 0.05 * (d_model ** -0.5) 
            
            # ìµœì¢… í•™ìŠµë¥  ë°˜í™˜
            return peak_lr_scale_factor * min(step ** -0.5, step * (warmup_steps ** -1.5))

        d_model = self.model_config.encoder.encoder_dim
        warmup_steps = self.optim_config.warmup_steps
        
        def lr_lambda(step):
            # LambdaLRì€ optimizerì˜ lrì— ì´ í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì„ ê³±í•˜ë¯€ë¡œ,
            # transformer_lr_schedule ìì²´ê°€ ì ˆëŒ€ í•™ìŠµë¥ ì„ ë°˜í™˜í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.
            # ë”°ë¼ì„œ optimizerì˜ lrì„ 1.0ìœ¼ë¡œ ì„¤ì •í•´ì•¼ ì´ ìŠ¤ì¼€ì¤„ì´ ì œëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
            return transformer_lr_schedule(step, d_model, warmup_steps)

        if self.optim_config.scheduling_type == "cosine-annealing":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer_config.num_epochs)
        elif self.optim_config.scheduling_type == "warmup":
            # ì´ "warmup" íƒ€ì…ì€ StepLRì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì›œì—… ìŠ¤ì¼€ì¤„ê³¼ ë‹¤ë¦…ë‹ˆë‹¤.
            # Conformer ë…¼ë¬¸ì˜ ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'lambda' íƒ€ì…ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
            logging.warning("Using 'warmup' scheduling_type with StepLR. This is NOT the Transformer LR schedule described in the Conformer paper. Consider using 'lambda' type.")
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.05)
        elif self.optim_config.scheduling_type == "lambda":
            # Conformer ë…¼ë¬¸ ìŠ¤ì¼€ì¤„ ì ìš©
            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
            
        self.scaler = torch.cuda.amp.GradScaler(
            init_scale=2**10,
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=2000
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step" # 'step' ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ëª…ì‹œ
            }
        } 
    