"""
ğŸ–¤ğŸ° JaeEun Baik, 2025
"""

import torch
import numpy as np
import pytorch_lightning as pl
import torch.nn as nn
import logging
import sentencepiece as spm

from modules.e2e_asr_model import e2eASR # ê¸°ì¡´ ASR ëª¨ë¸ ì„í¬íŠ¸

class PredictorModelModule(pl.LightningModule):
    def __init__(self, config, base_asr_model_ckpt_path: str):
        super().__init__()
        self.config = config
        self.base_asr_model_ckpt_path = base_asr_model_ckpt_path

        # 1. ê¸°ì¡´ ASR ëª¨ë¸ ë¡œë“œ
        logging.info(f"Loading base ASR model from checkpoint: {self.base_asr_model_ckpt_path}")
        
        self.model = e2eASR(config.asr, config.data.tokenizer)
        checkpoint = torch.load(base_asr_model_ckpt_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['state_dict'], strict=False) # strict=Falseë¡œ ìœ ì—°í•˜ê²Œ ë¡œë“œ

        for param in self.model.encoder.parameters():
            param.requires_grad = False
        logging.info("Encoder parameters frozen.")

        for param in self.model.joiner.parameters():
            param.requires_grad = False
        logging.info("Joiner parameters frozen.")
        
        self.criterion = nn.CrossEntropyLoss(ignore_index=self.model.eos) 
        self.sp_processor = spm.SentencePieceProcessor()
        self.sp_processor.load(config.data.tokenizer)
        self.char_list = [self.sp_processor.id_to_piece(i) for i in range(config.asr.decoder.odim)]

        self.save_hyperparameters(config) # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥

    def forward(self, input_tokens, input_lengths, target_tokens, target_lengths):
        pred_out, _ = self.model.predictor(y=input_tokens, y_lengths=input_lengths)
        
        if not hasattr(self, 'lm_head'):
            self.lm_head = nn.Linear(self.model.predictor.output_linear.out_features, self.model.odim).to(self.device)
        
        lm_logits = self.lm_head(pred_out) # (B, U_in, Vocab_size)

        # CrossEntropyLossëŠ” logitsì™€ targetì„ ë°›ìŠµë‹ˆë‹¤.
        # logits: (B, U_in, Vocab_size)
        # target_tokens: (B, U_target)
        
        # CE LossëŠ” 2D logitsì™€ 1D targetì„ ê¸°ëŒ€í•˜ë¯€ë¡œ flatten
        loss = self.criterion(lm_logits.view(-1, lm_logits.size(-1)), target_tokens.view(-1))
        
        return loss

    def training_step(self, batch, batch_idx):
        input_tokens, input_lengths, target_tokens, target_lengths = batch
        loss = self(input_tokens, input_lengths, target_tokens, target_lengths)
        self.log("train_lm_loss", loss, prog_bar=True, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx):
        input_tokens, input_lengths, target_tokens, target_lengths = batch
        loss = self(input_tokens, input_lengths, target_tokens, target_lengths)
        self.log("val_lm_loss", loss, prog_bar=True, sync_dist=True)
        # WER/CERì€ LM íŒŒì¸íŠœë‹ ë‹¨ê³„ì—ì„œëŠ” ë³´í†µ ê³„ì‚°í•˜ì§€ ì•ŠìŒ.
        # ì „ì²´ ASR ëª¨ë¸ë¡œ í‰ê°€í•´ì•¼ í•˜ë¯€ë¡œ ì´ ë‹¨ê³„ì—ì„œëŠ” ë¶ˆí•„ìš”.

    def configure_optimizers(self):
        # Predictor (ë° lm_head)ì˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.parameters()), # requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ë§Œ
            lr=np.float64(self.config.optimizer.op_lr), # LM íŒŒì¸íŠœë‹ì— ì‚¬ìš©í•  í•™ìŠµë¥  (ë‚®ê²Œ ì„¤ì •)
            betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-6
        )
        # LM íŒŒì¸íŠœë‹ì€ ë³´í†µ ê°„ë‹¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬ (ì˜ˆ: StepLR, CosineAnnealing) ë˜ëŠ” ê³ ì • LR ì‚¬ìš©
        # Conformer LR ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•œë‹¤ë©´ ì ì ˆíˆ ì¡°ì •
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.trainer.num_epochs)
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step" # 'step' ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            }
        }