"""
ğŸ–¤ğŸ° JaeEun Baik, 2025
"""

import os
import logging
import argparse
import yaml
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.strategies import DDPStrategy

from attrdict import AttrDict # config ë¡œë“œë¥¼ ìœ„í•´ í•„ìš”

from pred_datamodule import PredictorDataModule # ìƒˆ DataModule ì„í¬íŠ¸
from pred_modelmodule import PredictorModelModule # ìƒˆ ModelModule ì„í¬íŠ¸

# ê¸°ë³¸ ì„¤ì • (ê¸°ì¡´ trainer.pyì™€ ìœ ì‚¬)
torch.backends.cudnn.benchmark = True

class PredictorTrainer:
    def __init__(self, config, base_asr_model_ckpt: str):
        self.config = config
        self.base_asr_model_ckpt = base_asr_model_ckpt # íŒŒì¸íŠœë‹í•  ASR ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ

        # DataModule ì´ˆê¸°í™”
        self.datamodule = PredictorDataModule(config)
        
        # Predictor íŒŒì¸íŠœë‹ ModelModule ì´ˆê¸°í™”
        self.model = PredictorModelModule(config, self.base_asr_model_ckpt)
        
        # ë¡œê¹… ì„¤ì •
        logger = WandbLogger(
            project=config.trainer.proj + "_Predictor_Finetune", # í”„ë¡œì íŠ¸ ì´ë¦„ ë³€ê²½
            name=config.trainer.exp_name + "_Predictor_Finetune" # ì‹¤í—˜ ì´ë¦„ ë³€ê²½
        )
        
        # ì½œë°± ì„¤ì •
        callbacks = self._setup_callbacks()
        strategy = DDPStrategy(find_unused_parameters=False) # DDP ì „ëµ ì‚¬ìš©
        
        trainer_kwargs = {
            'max_epochs': config.trainer.num_epochs, # ì—í­ ìˆ˜ëŠ” ê¸°ì¡´ ASRë³´ë‹¤ ì ê²Œ ì„¤ì • (íŒŒì¸íŠœë‹)
            'accelerator': 'gpu' if config.trainer.gpus > 0 else 'cpu',
            'devices': config.trainer.gpus if config.trainer.gpus > 0 else None,
            'logger': logger,
            'callbacks': callbacks,
            'log_every_n_steps': config.trainer.log_every_n_steps,
            'val_check_interval': config.trainer.val_check_interval,
            'precision': config.trainer.precision, # BF16/FP16 ì •ë°€ë„
            'accumulate_grad_batches': config.trainer.accumulate_grad_batches,
            'gradient_clip_val': config.trainer.gradient_clip_val,
            'strategy': strategy,
            'reload_dataloaders_every_n_epochs': config.trainer.reload_dataloaders_every_n_epochs,
        }

        self.trainer = pl.Trainer(**trainer_kwargs)

    def _setup_callbacks(self):
        callbacks = []
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ (LM íŒŒì¸íŠœë‹ìš© ë³„ë„ ê²½ë¡œ)
        checkpoint_dir = os.path.join(os.path.dirname(self.config.checkpoint.model_save_path), "predictor_finetune")
        os.makedirs(checkpoint_dir, exist_ok=True) # ë””ë ‰í† ë¦¬ ìƒì„±

        callbacks.append(
            ModelCheckpoint(
                dirpath=checkpoint_dir,
                filename='{epoch:02d}-{val_lm_loss:.4f}', # LM lossë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
                monitor='val_lm_loss', # val_lm_lossê°€ ê°€ì¥ ë‚®ì„ ë•Œ ì €ì¥
                save_top_k=self.config.checkpoint.save_top_k,
                mode='min',
                save_last=True
            )
        )
        callbacks.append(LearningRateMonitor(logging_interval='step'))
        return callbacks

    def finetune(self):
        logging.info(f"Starting Predictor finetuning for {self.config.trainer.num_epochs} epochs")
        # resume_from_checkpointëŠ” íŒŒì¸íŠœë‹ ìì²´ë¥¼ ì´ì–´ê°ˆ ë•Œ ì‚¬ìš©
        self.trainer.fit(self.model, self.datamodule, ckpt_path=self.config.trainer.get('resume_finetune_ckpt_path', None))
        
        # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        best_model_path = self.trainer.checkpoint_callback.best_model_path
        if best_model_path:
            logging.info(f"Best finetuned predictor model saved at: {best_model_path}")
            # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œë¥¼ ë³„ë„ íŒŒì¼ì— ì €ì¥
            finetune_model_path_file = os.path.join(os.path.dirname(self.config.checkpoint.model_save_path), 'best_finetuned_predictor_path.txt')
            with open(finetune_model_path_file, 'w') as f:
                f.write(best_model_path)

    @staticmethod
    def load_config(config_path):
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        logging.info(f"Loaded configuration from: {config_path}")
        return AttrDict(config)

def main():
    parser = argparse.ArgumentParser(description="Predictor Finetuning Script for RNN-T ASR")
    parser.add_argument('--config', type=str, required=True, help='Path to configuration YAML file for finetuning.')
    parser.add_argument('--base_asr_ckpt', type=str, required=True,
                        help='Path to the base ASR model checkpoint to finetune its predictor.')
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì • (ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    config = PredictorTrainer.load_config(args.config)
    
    finetuner = PredictorTrainer(config, args.base_asr_ckpt)
    finetuner.finetune()

if __name__ == "__main__":
    main()