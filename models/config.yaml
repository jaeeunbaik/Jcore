dataloader:
  batch_size: 8
  num_workers: 1
  pin_memory: False
data:
  scp_dir: /home/hdd2/jenny/Self-Distillation-ASR/scp/
  sample_rate: 16000
  n_mels: 80
  n_fft: 400
  win_length: 400
  hop_length: 160
  test_clean: True
  augmentation:
    noise_mixing: false
    noise_dir: /home/hdd2/jenny/Self-Distillation-ASR/scp/noise.scp
    noise_prob: 0.5
    noise_level: 0.1

    rir_mixing: false
    rir_dir: "/home/hdd2/jenny/Self-Distillation-ASR/scp/rir.scp"
    rir_prob: 0.5
    RT_list: [0.3, 0.5, 0.7]

    specaugment: true
    time_mask_param: 70
    freq_mask_param: 15
    n_time_masks: 2
    n_freq_masks: 2
  
    speed_perturb: false
    speed_factors: [0.9, 1.0, 1.1]  # 0.9가 33%로 적용, 1.0이 33%로 적용, 1.1이 33%로 적용
    speed_prob: 0.5
  tokenizer: /home/hdd2/jenny/Self-Distillation-ASR/util/spm/unigram/unigram5000.model
model:
  asr:
    encoder:
      type: "Conformer"
      sampling_in: 80
      subsampling: "conv"  # dwconv, conv
      subsampling_factor: 4  # subsampling to 1/(subsampling_factor) length.
      subsampling_channels: 128
      num_blocks: 17
      input_size: 256
      output_size: 512
      dropout_rate: 0.1
      pos_embed_type: "relpos"  # relpos, abspos
      ff_activation_type: "swish"
      ff_expansion_factor: 4
      cnn_module_kernel: 9
      attention_heads: 8
      linear_units: 2048
      return_intermediates: False
      normalize_before: False
    decoder:
      beam_size: 1  # if ctc only, decoding method is greedy search (= no beam search)
      penalty: 0.1  # < 0: 너무 짧은 출력 억제, 0.0 ~ 0.2 : 일반적인 ASR, 0.3 이상: 너무 긴 출력 억제
      minlenratio: 0.1
      maxlenratio: 1.0 # 디코딩 출력의 최대 길이, len(encoder) * maxlenratio
      rnnlm: false
      lm_weight: 0.2
      nbest: 1  # beam search 결과 중에서 몇 개 가져올 지 결정
      type: "ctc"  # ctc, rnnt, transformer, hybrid
      ctc_type: "builtin"  # builtin, cudnnctc, gtnctc
      ctc_weight: 1.0
      odim: 1024
      eprojs: 512
      reduce: True
      dropout_rate: 0.0
      loss_function: "CE"  # KL_Div, CE
      lsm_weight: 0.1
    sym_space: " "
    sym_blank: "<blank>"
    report_cer: False
    report_wer: False
  distillation:
    using_distillation: False
    temperature: 2.0
    alpha: 0.5
  teacher:
    encoder:
      type: "FastConformer"
      layers: 12
      hidden_size: 512
      dropout: 0.1
      attention_heads: 8
      return_intermediates: True
    decoder:
      type: "ctc"  # ctc, rnnt, transformer, hybrid
      ctc_weight: None
      loss_function: "CE"  # KL_Div, CE
      lsm_weight: 0.1
  student:
    encoder:
      type: "FastConformer"
      layers: 12
      hidden_size: 512
      dropout: 0.1
      attention_heads: 8
      return_intermediates: True
    decoder:
      type: "CTC"  # CTC, RNNT, Transformer, Hybrid
      ctc_weight: 1.0
      loss_function: "CE"  # KL_Div, CE
      lsm_weight: 0.1
      char_list: [" ", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
            "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z", "'"]
      sym_space: " "
      sym_blank: "<blank>"
      report_cer: false
      report_wer: false
trainer:
  proj: "noise-robust-ASR"
  exp_name: "clean-ASR"
  log_dir: "logs"
  num_epochs: 100
  weight_decay: 0.0001
  warmup_epochs: 10
  precision: 32
  gpus: 2
  gradient_clip_val: 5.0
  resume_from_checkpoint: false
  log_every_n_steps: 100
  val_check_interval: 1.0
  strategy: "ddp"  # ddp, ddp_spawn, deepspeed
  reload_dataloaders_every_n_epochs: 4
  # ckpt_path: null
checkpoint:
  checkpoint_monitor: "val_loss"
  save_top_k: 3
  model_save_path: "models/clean_ASR.pth"
optimizer:
  type: "Adam"
  op_lr: 1e-4
  max_epochs: 100
  scheduling_type: "cosine-annealing"  # cosine-annealing, warmuplr