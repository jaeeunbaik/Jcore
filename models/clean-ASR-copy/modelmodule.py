"""
ğŸ–¤ğŸ° JaeEun Baik, 2025
"""
import re
import random 

import jiwer
import torch
import wandb
import numpy as np
import pytorch_lightning as pl
from typing import List

import sentencepiece as spm

from util.utils_text import TokenProcessor, ErrorCalculator, preprocess_text
from modules.loss.kd_loss import KDLoss
from modules.kd_wrapper import KDWrapper
from modules.e2e_asr_model import e2eASR



class ModelModule(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.model_config = config.model.asr
        self.kd_config = config.model.distillation
        self.use_kd = self.kd_config.using_distillation
        # self.kd_loss = KDLoss(model_config.distillation)
        # if self.use_kd:
        #     self.teacher_model = e2eASR(model_config.teacher)
        #     self.student_model = e2eASR(model_config.student)
            
        #     self.model = KDWrapper(self.teacher_model, self.student_model, self.kd_loss, model_config.distillation.target)
        # else:
        self.optim_config = config.optimizer
        self.lr = np.float64(self.optim_config.op_lr)
        self.tokenizer_path = config.data.tokenizer
        self.token_processor = TokenProcessor(config.data.tokenizer)
        self.trainer_config = config.trainer
        self.model = e2eASR(self.model_config, self.tokenizer_path)
        
        self.sp_processor = spm.SentencePieceProcessor()
        self.sp_processor.load(self.tokenizer_path)
        
        self.char_list = [self.sp_processor.id_to_piece(i) for i in range(self.model_config.decoder.odim)]
        self.error_calculator = ErrorCalculator(
            char_list=self.char_list,
            sym_space=self.model_config.sym_space,
            sym_blank=self.model_config.sym_blank,
            report_cer=self.model_config.report_cer,
            report_wer=self.model_config.report_wer
        )
        
    def log_gradient_norms(self):
        total_norm = 0.0
        module_norms = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None:
                module = name.split('.')[0]
                norm = param.grad.norm().item()
                if module not in module_norms:
                    module_norms[module] = []
                module_norms[module].append(norm)
                total_norm += norm ** 2
        total_norm = total_norm ** 0.5
        
        # ë¡œê·¸ ê¸°ë¡
        self.log("grad/total_norm", total_norm, prog_bar=False)
        for module, norms in module_norms.items():
            avg_norm = sum(norms) / len(norms)
            self.log(f"grad/{module}_norm", avg_norm, prog_bar=False)
    
    def print_model_structure(self):
        """ëª¨ë¸ êµ¬ì¡° ë° íŒŒë¼ë¯¸í„° ì¶œë ¥"""
        print("\n===== ëª¨ë¸ êµ¬ì¡° =====")
        total_params = 0
        for name, module in self.model.named_modules():
            if len(list(module.children())) == 0:  # ë§ë‹¨ ëª¨ë“ˆë§Œ ì¶œë ¥
                params = sum(p.numel() for p in module.parameters())
                total_params += params
                print(f"{name}: {module.__class__.__name__}, íŒŒë¼ë¯¸í„° ìˆ˜: {params:,}")
        
        print(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        
        # encoderì™€ ctc ëª¨ë“ˆ í™•ì¸
        print("\n===== ì£¼ìš” ëª¨ë“ˆ í™•ì¸ =====")
        if hasattr(self.model, 'encoder'):
            print(f"Encoder type: {type(self.model.encoder)}")
        else:
            print("Encoder not found!")
            
        if hasattr(self.model, 'ctc'):
            print(f"CTC type: {type(self.model.ctc)}")
        else:
            print("CTC module not found!")    

    # ëª¨ë¸ êµ¬ì¡°ë‘ íŒŒë¼ë¯¸í„°ë‘ íŒŒë¼ë¯¸í„°ìˆ˜ ì¶œë ¥í• ë ¤ë©´
    def on_train_start(self):
        """í›ˆë ¨ ì‹œì‘ ì‹œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ"""
        super().on_train_start()
        # self.print_model_structure()
        # ì²« ë²ˆì§¸ ë°°ì¹˜ì— ëŒ€í•´ ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° í™•ì¸
        # print("\n==== ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° í™•ì¸ ì‹œì‘ ====")
    #     self._parameter_debugging_done = False

    def on_train_batch_end(self, batch_output, batch, batch_idx):
        """ê° ë°°ì¹˜ ì‹œì‘ ì‹œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ"""
        # ì²« ë°°ì¹˜ì—ì„œë§Œ ë””ë²„ê¹… ì‹¤í–‰
        if batch_idx == 0 and not getattr(self, '_parameter_debugging_done', False):
            x, x_len, y = batch
            # ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì‚¬ìš©
            if x.size(0) > 1:
                x = x[:1]
                x_len = x_len[:1]
                if y is not None and not isinstance(y, dict):
                    y = y[:1] if len(y.shape) > 0 else y
            
            # print("\në””ë²„ê¹… ìƒ˜í”Œ í˜•íƒœ:")
            # print(f"x: {x.shape}, x_len: {x_len.shape}")
            if y is not None:
                if isinstance(y, dict):
                    y_shape = {k: v.shape for k, v in y.items()}
                else:
                    y_shape = y.shape
                # print(f"y: {y_shape}")
            
            # íŒŒë¼ë¯¸í„° ì‚¬ìš© ìƒíƒœ í™•ì¸
            # _, _ = self.check_unused_parameters(x, x_len, y)
            # self._parameter_debugging_done = True
            
            # print("\n==== ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° í™•ì¸ ì™„ë£Œ ====\n")
    
    def training_step(self, batch, batch_idx):
        x, x_len, y = batch
        if self.use_kd:
            loss_dict = self.model(x, x_len, y)
            self.log("train/total_loss", loss_dict["total_loss"])
            self.log("train/kd_loss", loss_dict["kd_loss"])
            self.log("train/asr_loss", loss_dict["student_loss"])
            return loss_dict["total_loss"]
        else:
            loss = self.model(x, x_len, y)
            log_items = {
                "train/total_loss": loss.get("loss"),
                "train/ctc_loss": loss.get("loss_ctc"),
                "train/cer": loss.get("cer"),
                "train/wer": loss.get("wer"),
            }
            
            # Noneì´ ì•„ë‹Œ ê°’ë§Œ logë¡œ ë„˜ê¹€
            for key, value in log_items.items():
                if value is not None:
                    self.log(key, value, prog_bar=True, sync_dist=True)
            ctc_probs = self.model.calculate_all_ctc_probs(x, x_len, y)
            if ctc_probs is not None:
                confidence = torch.tensor(ctc_probs).max(-1)[0].mean().item()
                self.log("train/ctc_confidence", confidence, prog_bar=True)

            # --- Attention visualization ---
            attn_weights = self.model.calculate_all_attentions(x, x_len, y)
            if "decoder.0.self_attn" in attn_weights:
                attn_matrix = attn_weights["decoder.0.self_attn"][0]
                self.logger.experiment.add_image(
                    "train/attention", torch.tensor(attn_matrix).mean(0, keepdim=True), self.global_step
                )

                self.log_gradient_norms()
            return loss.get("loss")


    def validation_step(self, batch, batch_idx):
        x, x_len, y = batch
        if self.use_kd:
            loss_dict = self.model(x, x_len, y)
            self.log("val/loss", loss_dict["total_loss"])
            self.log("val/kd_loss", loss_dict["kd_loss"])
            self.log("val/asr_loss", loss_dict["student_loss"])
        else:
            loss_output = self.model(x, x_len, y) # `loss` ë³€ìˆ˜ëª…ì„ `loss_output`ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í˜¼ë™ ë°©ì§€
            log_items = {
                "val/loss": loss_output.get("loss"),
                "val/ctc_loss": loss_output.get("loss_ctc"),
                "val/att_loss": loss_output.get("loss_att"),
            }

            # Noneì´ ì•„ë‹Œ ê°’ë§Œ logë¡œ ë„˜ê¹€
            for key, value in log_items.items():
                if value is not None:
                    self.log(key, value, prog_bar=True, sync_dist=True)
            
            # CTC Confidence ë¡œê¹… (í•„ìš”í•˜ë‹¤ë©´)
            ctc_probs = self.model.calculate_all_ctc_probs(x, x_len, y)
            if ctc_probs is not None:
                confidence = torch.tensor(ctc_probs).max(-1)[0].mean().item()
                self.log("val/ctc_confidence", confidence, prog_bar=True)

            # Attention visualization (í•„ìš”í•˜ë‹¤ë©´)
            attn_weights = self.model.calculate_all_attentions(x, x_len, y)
            if "decoder.0.self_attn" in attn_weights:
                attn_matrix = attn_weights["decoder.0.self_attn"][0]
                self.logger.experiment.add_image(
                    "val/attention", torch.tensor(attn_matrix).mean(0, keepdim=True), self.global_step
                )
            # --- ëª¨ë¸ ë””ì½”ë”© ë° WER/CER ê³„ì‚° (ìƒˆë¡œìš´ ë¡œì§) ---
            with torch.no_grad():
                # self.model_config.decoderëŠ” Namespace ê°ì²´ë¡œ, í•„ìš”í•œ recog_argsë¥¼ í¬í•¨í•´ì•¼ í•¨.
                # (ì˜ˆ: beam_size, maxlenratio, minlenratio, lm_weight ë“±)
                recog_args = self.model_config.decoder 
                decoded_raw_output = self.model.recognize(x, x_len, y, recog_args=recog_args)
            
            # ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹´ì„ ë³€ìˆ˜
            predicted_transcriptions: List[str] = []
            # ë””ì½”ë” íƒ€ì…ì— ë”°ë¼ decoded_raw_output ì²˜ë¦¬
            if self.model.decoder_type in ['ctc', 'rnnt']:
                # ctcì™€ rnnt ë””ì½”ë”©ì€ recognizeì—ì„œ ì´ë¯¸ List[str]ì„ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •í–ˆë‹¤ê³  ê°€ì •
                if isinstance(decoded_raw_output, list) and all(isinstance(d, str) for d in decoded_raw_output):
                    predicted_transcriptions = decoded_raw_output
                else:
                    if isinstance(decoded_raw_output, dict) and decoded_raw_output:
                        first_key = next(iter(decoded_raw_output))
                        if isinstance(decoded_raw_output[first_key], list) and all(isinstance(s, str) for s in decoded_raw_output[first_key]):
                            predicted_transcriptions = decoded_raw_output[first_key]
                        else:
                            self.log_text("val_debug/decode_type_mismatch", "RNNT/CTC output is not List[str] as expected.", self.global_step)
                            predicted_transcriptions = [""] * len(x) # ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
                    else:
                        self.log_text("val_debug/decode_type_mismatch", "RNNT/CTC output is not List[str] or Dict as expected.", self.global_step)
                        predicted_transcriptions = [""] * len(x) # ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
            else: # ê·¸ ì™¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë””ì½”ë” íƒ€ì…
                self.log_text("val_debug/unsupported_decoder_type", f"Unsupported decoder type: {self.model.decoder_type}", self.global_step)
                predicted_transcriptions = [""] * len(x) # ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€

            # Ground Truth í…ìŠ¤íŠ¸ ë³€í™˜ (ëª¨ë“  ë””ì½”ë” íƒ€ì…ì— ê³µí†µ)
            reference_transcriptions: List[str] = []
            for i in range(len(y)):
                gt_tokens = y[i].tolist()
                gt_text = self.token_processor.id2text(gt_tokens, filter_blank=True) # TokenProcessorê°€ ì˜ ì²˜ë¦¬í•˜ëŠ”ì§€ ì¤‘ìš”!
                reference_transcriptions.append(gt_text)
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (WER ê³„ì‚°ì„ ìœ„í•´ ê³µí†µì ìœ¼ë¡œ ìˆ˜í–‰)
            # preprocess_text í•¨ìˆ˜ê°€ util/utils_textì— ìˆë‹¤ê³  ê°€ì •
            processed_predicted_transcriptions = [preprocess_text(text) for text in predicted_transcriptions]
            processed_reference_transcriptions = [preprocess_text(text) for text in reference_transcriptions]

            batch_wers = []
            for gt, pred in zip(processed_reference_transcriptions, processed_predicted_transcriptions):
                if gt and pred: # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ ë•Œë§Œ WER ê³„ì‚°
                    wer = jiwer.wer(gt, pred)
                    batch_wers.append(wer)
                else:
                    # ë¹ˆ í…ìŠ¤íŠ¸ë¡œ ì¸í•´ WER ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° (ë¡œê·¸ ì¶œë ¥)
                    self.log_text("val_debug/empty_text_for_wer", f"Empty text for WER calculation: GT='{gt}', PR='{pred}'", self.global_step)
            # í‰ê·  WER ê³„ì‚° ë° ë¡œê¹…
            if batch_wers:
                avg_wer = sum(batch_wers) / len(batch_wers)
                self.log("val_wer", avg_wer, prog_bar=True, sync_dist=True)
            else:
                self.log("val_wer", 1.0, prog_bar=True, sync_dist=True) # ëª¨ë“  WER ê³„ì‚° ë¶ˆê°€ëŠ¥ ì‹œ 1.0 (ìµœì•…) ë¡œê¹…

            # ê° ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            if batch_idx == 0 and len(processed_reference_transcriptions) > 0:
                print(f"\n===== ë°°ì¹˜ {batch_idx}, ìƒ˜í”Œ 0 (Decoder Type: {self.model.decoder_type}) =====")
                print(f"GT: '{processed_reference_transcriptions[0]}'")
                print(f"PR: '{processed_predicted_transcriptions[0]}'")
                if batch_wers:
                    print(f"WER: {batch_wers[0]:.4f} ({batch_wers[0] * 100:.2f}%)")
                else:
                    print("WER ê³„ì‚° ë¶ˆê°€ (ë¹ˆ í…ìŠ¤íŠ¸)")
                    
                    
            # loss = self.model(x, x_len, y)
            # log_items = {
            #     "val/loss": loss.get("loss"),
            #     "val/ctc_loss": loss.get("loss_ctc"),
            #     "val/att_loss": loss.get("loss_att"),
            #     "val/wer": loss.get("wer"),
            #     "val/cer": loss.get("cer"),
            # }

            # # Noneì´ ì•„ë‹Œ ê°’ë§Œ logë¡œ ë„˜ê¹€
            # for key, value in log_items.items():
            #     if value is not None:
            #         self.log(key, value, prog_bar=True, sync_dist=True)
            # ctc_probs = self.model.calculate_all_ctc_probs(x, x_len, y)
            # if ctc_probs is not None:
            #     confidence = torch.tensor(ctc_probs).max(-1)[0].mean().item()
            #     self.log("val/ctc_confidence", confidence, prog_bar=True)

            # # --- Attention visualization ---
            # attn_weights = self.model.calculate_all_attentions(x, x_len, y)
            # if "decoder.0.self_attn" in attn_weights:
            #     attn_matrix = attn_weights["decoder.0.self_attn"][0]
            #     self.logger.experiment.add_image(
            #         "val/attention", torch.tensor(attn_matrix).mean(0, keepdim=True), self.global_step
            #     )


            # # ëª¨ë¸ ë””ì½”ë”©
            # with torch.no_grad():
            #     decoded = self.model.recognize(x, x_len, y, recog_args=self.model_config.decoder)

            # batch_wers = []

            # if decoded:  # decoded = [{'score': score, 'yseq': [2, token1, ...]}, ... ]
            #     for i in range(len(decoded)):  # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°˜ë³µ
            #         # print(f"[DEBUG] decoded ì •ë³´ len(decoded[0]['yseq']) : {len(decoded[0]['yseq'])}")
            #         # print(f"[DEBUG] decoded ì •ë³´ len(decoded[0]['yseq'][1:]) : {len(decoded[0]['yseq'][1:])}")
            #         # print(f"[DEBUG] decoded ì •ë³´ decoded[0]['yseq'][1:][i] : {decoded[0]['yseq'][1:][i]}")
            #         if 'yseq' in decoded[i]:
            #             yseq = decoded[i]['yseq'][1:]  # SOS í† í° ì œì™¸
            #             pred_text = self.token_processor.id2text(yseq, filter_blank=True) if yseq else ""

            #             # Ground Truth í…ìŠ¤íŠ¸ ë³€í™˜
            #             gt_tokens = y[i].tolist() if i < len(y) else []
            #             gt_text = self.token_processor.id2text(gt_tokens, filter_blank=True)

            #             # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            #             pred_text = preprocess_text(pred_text)
            #             gt_text = preprocess_text(gt_text)

            #             # WER ê³„ì‚°
            #             if gt_text and pred_text:
            #                 wer = jiwer.wer(gt_text, pred_text)
            #                 batch_wers.append(wer)

            #                 # ê° ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡ ë° ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶œë ¥
            #                 if i == 0:  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
            #                     print(f"\n===== ë°°ì¹˜ {batch_idx}, ìƒ˜í”Œ {i} =====")
            #                     print(f"GT: '{gt_text}'")
            #                     print(f"PR: '{pred_text}'")
            #                     print(f"WER: {wer:.4f} ({wer * 100:.2f}%)")
            #             else:
            #                 if i == 0:  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
            #                     print(f"\n===== ë°°ì¹˜ {batch_idx}, ìƒ˜í”Œ {i} =====")
            #                     print("ë¹ˆ í…ìŠ¤íŠ¸ê°€ ìˆì–´ WERë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            #     # í‰ê·  WER ê³„ì‚° ë° ë¡œê¹…
            #     if batch_wers:
            #         avg_wer = sum(batch_wers) / len(batch_wers)
            #         self.log("val_wer", avg_wer, prog_bar=True)
            #     else:
            #         self.log("val_wer", 0, prog_bar=True)
                                
                
                
    def on_validation_epoch_start(self):
        """ì—í­ ì‹œì‘ ì‹œ WER í†µê³„ ì´ˆê¸°í™”"""
        self.val_wer_samples = []
        self.val_wer_sum = 0
        self.val_wer_count = 0

    def on_validation_epoch_end(self):
        """ì—í­ ì¢…ë£Œ ì‹œ WER í†µê³„ ì²˜ë¦¬ ë° ë¡œê¹…"""
        if self.val_wer_count > 0:
            avg_wer = self.val_wer_sum / self.val_wer_count
            self.log("val_wer", avg_wer)
            
            # íˆìŠ¤í† ê·¸ë¨ ë¡œê¹…
            if self.logger and hasattr(self.logger, "experiment"):
                import numpy as np
                import wandb
                if self.val_wer_samples:
                    self.logger.experiment.log({
                        "val_wer_histogram": wandb.Histogram(np.array(self.val_wer_samples)),
                        "global_step": self.global_step
                    })
            
            print(f"\n===== ê²€ì¦ ì™„ë£Œ =====")
            print(f"ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {self.val_wer_count}")
            print(f"í‰ê·  WER: {avg_wer:.4f} ({avg_wer*100:.2f}%)")    
                
                
    def test_step(self, batch, batch_idx):
        x, x_len, y = batch
        if self.use_kd:
            logits = self.student_model.encode(x, x_len)
            decoded_trans = self.student_model.recognize(logits)
        else:
            decoded_trans = self.model.recognize(x, x_len, y, self.model_config.decoder)

        ref_trans: List[str] = []
        for single_y_tokens in y:
            # ignore_id (padding)ë¥¼ ì œì™¸í•˜ê³  ì‹¤ì œ í† í°ë§Œ ì‚¬ìš©
            # self.model.sos, self.model.eos ë„ ê³ ë ¤í•˜ì—¬ ì œê±°í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì´ ë¶€ë¶„ì€ í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ êµ¬ì„±ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            filtered_tokens = [
                token.item() for token in single_y_tokens 
                if token.item() != self.model.ignore_id and 
                   token.item() != self.model.sos and 
                   token.item() != self.model.eos
            ]
            ref_trans.append(self.token_processor.id2text(filtered_tokens))
        
        cer_batch = self.error_calculator.calculate_cer(decoded_trans, ref_trans)
        wer_batch = self.error_calculator.calculate_wer(decoded_trans, ref_trans)
        
        # í‰ê·  CER/WER ë¡œê¹… (ë°°ì¹˜ ë‹¨ìœ„)
        self.log("test_cer", cer_batch, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log("test_wer", wer_batch, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        # í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ì ì¸ ë¡œê¹… ë˜ëŠ” ê²°ê³¼ ë°˜í™˜
        return {
            "decoded_transcriptions": decoded_trans,
            "reference_transcriptions": ref_trans,
            "cer_batch": cer_batch,
            "wer_batch": wer_batch
        }
        
        # # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        # self.token_processor = TokenProcessor(self.tokenizer_path)
        # id2text = self.token_processor.id2text
        
        # # ê²°ê³¼ í™•ì¸
        # if decoded and len(decoded) > 0:
        #     yseq = decoded[0]['yseq']
            
        #     # í…ìŠ¤íŠ¸ ë³€í™˜ (SOS í† í° ì œì™¸)
        #     if len(yseq) > 1:
        #         pred_tokens = yseq[1:]
                
        #         # ê° í† í°ì„ ê°œë³„ì ìœ¼ë¡œ ë””ì½”ë”©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        #         token_texts = []
        #         for token in pred_tokens:
        #             if torch.is_tensor(token):
        #                 token_id = token.item() if token.numel() == 1 else token.tolist()
        #             else:
        #                 token_id = token
                        
        #             # ê° í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        #             token_text = id2text([token_id] if isinstance(token_id, int) else token_id)
        #             token_texts.append(token_text)
                
        #         # í† í°ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ê³  í›„ì²˜ë¦¬
        #         raw_text = " ".join(token_texts)
                
        #         # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        #         cleaned_text = re.sub(r'\s+', ' ', raw_text)                  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        #         cleaned_text = re.sub(r'\s([,.!?:;])', r'\1', cleaned_text)   # ë¬¸ì¥ ë¶€í˜¸ ì• ê³µë°± ì œê±°
        #         cleaned_text = cleaned_text.strip()                           # ì•ë’¤ ê³µë°± ì œê±°
                
        #         # SentencePieceì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” íŠ¹ìˆ˜ ì²˜ë¦¬
        #         cleaned_text = re.sub(r'\s+â–', ' ', cleaned_text)  # SentencePiece í† í° íŠ¹ìˆ˜ ì²˜ë¦¬
        #         cleaned_text = cleaned_text.replace('â–', ' ')      # SentencePiece ë§ˆì»¤ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        #         cleaned_text = re.sub(r'\s+', ' ', cleaned_text)   # ë‹¤ì‹œ ì—°ì†ëœ ê³µë°± ì œê±°
                
        #         # GT í† í° í•„í„°ë§ ë° í…ìŠ¤íŠ¸ ë³€í™˜
        #         try:
        #             # Ground Truth í…ìŠ¤íŠ¸ ì²˜ë¦¬
        #             gt_tokens_raw = y[0].tolist() if y.dim() > 1 else y.tolist()
                    
        #             # -1 ë° íŠ¹ìˆ˜ í† í° í•„í„°ë§ (0: padding, 1: sos, 2: eos, -1: ignore_id)
        #             valid_tokens = []
        #             for t in gt_tokens_raw:
        #                 # ìœ íš¨í•œ í† í° ë²”ìœ„ í™•ì¸ (í† í¬ë‚˜ì´ì € ì‚¬ì „ í¬ê¸°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        #                 max_token_id = self.token_processor.sp.get_piece_size() - 1
                        
        #                 # ìœ íš¨í•œ ë²”ìœ„ì˜ í† í°ë§Œ í¬í•¨
        #                 if 0 <= t <= max_token_id:
        #                     valid_tokens.append(t)
        #                 elif t == -1:
        #                     # -1 í† í°(ignore_id)ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        #                     continue
        #                 else:
        #                     # ì˜ˆìƒì¹˜ ëª»í•œ í† í° ID ë””ë²„ê¹…
        #                     self.print(f"[WARNING] ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ í† í° ID: {t}")
                    
        #             # í•„í„°ë§ëœ í† í°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
        #             if valid_tokens:
        #                 gt_text = id2text(valid_tokens)
        #             else:
        #                 gt_text = "[í† í° ì—†ìŒ]"
                    
        #         except Exception as e:
        #             self.print(f"[ERROR] GT í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        #             gt_text = "[ì²˜ë¦¬ ì˜¤ë¥˜]"
                
        #         # ì˜ˆì‹œ ì¶œë ¥
        #         self.print(f"\n--- í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ {batch_idx} ---")
        #         self.print(f"GT: {gt_text}")
        #         self.print(f"PR: {cleaned_text}")
                
        #         # WER ê³„ì‚° (jiwer ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
        #         try:
        #             import jiwer
                    
        #             # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        #             gt_processed = preprocess_text(gt_text)
        #             pr_processed = preprocess_text(cleaned_text)
                    
        #             # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ ê³„ì‚° ìƒëµ
        #             if len(gt_processed.strip()) > 0 and len(pr_processed.strip()) > 0:
        #                 # WER ê³„ì‚°
        #                 sample_wer = jiwer.wer(gt_processed, pr_processed)
                        
        #                 # WERì„ ë¡œê·¸ë¡œ ì €ì¥
        #                 self.log(f"test_sample_wer", sample_wer, on_step=True)
                        
        #                 # í†µê³„ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        #                 if not hasattr(self, 'wer_sum'):
        #                     self.wer_sum = 0.0
        #                     self.wer_count = 0
                        
        #                 self.wer_sum += sample_wer
        #                 self.wer_count += 1
                        
        #                 # í˜„ì¬ í‰ê·  WER ê³„ì‚° ë° ë¡œê¹…
        #                 current_avg_wer = self.wer_sum / self.wer_count
        #                 self.log("test_avg_wer", current_avg_wer, on_step=True)
                        
        #                 # WER ì¶œë ¥
        #                 self.print(f"WER: {sample_wer:.4f} ({sample_wer*100:.2f}%)")
        #                 self.print(f"í˜„ì¬ê¹Œì§€ í‰ê·  WER: {current_avg_wer:.4f} ({current_avg_wer*100:.2f}%)")
        #                 self.print(f"ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜: {self.wer_count}")
        #             else:
        #                 self.print("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ WER ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    
        #         except ImportError:
        #             self.print("jiwer ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ WER ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤. 'pip install jiwer'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        #         except Exception as e:
        #             self.print(f"WER ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
                
        #         return {"text": cleaned_text, "wer": current_avg_wer}
        
        # # ë””ì½”ë”© ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
        # self.print(f"\n--- í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ {batch_idx} ---")
        # self.print("ë””ì½”ë”© ì‹¤íŒ¨")
        # return {"text": "ë””ì½”ë”© ì‹¤íŒ¨"}

            
    def configure_optimizers(self):
        # optimizer
        if self.optim_config.type == "AdamW":
            optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-6)
        elif self.optim_config.type == "Adam":
            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
            
        # scheduler
        def transformer_lr_schedule(step, d_model, warmup_steps):
            """
            Transformer Learning Rate Schedule.

            Args:
                step (int): í˜„ì¬ ìŠ¤í….
                d_model (int): ëª¨ë¸ì˜ ì°¨ì› (hidden size).
                warmup_steps (int): ì›Œë°ì—… ìŠ¤í… ìˆ˜.

            Returns:
                float: í•™ìŠµë¥  ìŠ¤ì¼€ì¼ë§ ê°’.
            """
            if step == 0:
                step = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
            scale = d_model ** -0.5
            return scale * min(step ** -0.5, step * (warmup_steps ** -1.5))
        d_model = self.model_config.encoder.encoder_dim
        warmup_steps = self.optim_config.warmup_steps
        def lr_lambda(step):
            return transformer_lr_schedule(step, d_model, warmup_steps)

        if self.optim_config.scheduling_type == "cosine-annealing":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer_config.num_epochs)  # T_max : cosine ì£¼ê¸° í•œ ë²ˆ ë„ëŠ”ë° ê±¸ë¦¬ëŠ”
        elif self.optim_config.scheduling_type == "warmup":
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.05)
        elif self.optim_config.scheduling_type == "lambda":
            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
            
        self.scaler = torch.cuda.amp.GradScaler(
            init_scale=2**10,
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=2000
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step"
            }
        }
            
            
    def check_unused_parameters(self, x, x_len, y):
        """
        ëª¨ë¸ì˜ ì‚¬ìš© ë° ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
        """
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.model.train()
        self.model.zero_grad()
        
        # íŒŒë¼ë¯¸í„° ì´ë¦„ê³¼ requires_grad ìƒíƒœ ì €ì¥
        param_status_before = {}
        for name, param in self.model.named_parameters():
            param_status_before[name] = {
                'requires_grad': param.requires_grad,
                'grad': param.grad,
            }
        
        # í¬ì›Œë“œ ë° ë°±ì›Œë“œ íŒ¨ìŠ¤ ìˆ˜í–‰
        loss = self.model(x, x_len, y)
        if isinstance(loss, dict):
            loss_value = loss.get('loss')
            if loss_value is not None:
                loss_value.backward()
        else:
            loss.backward()
        
        # ì‚¬ìš©/ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° í™•ì¸
        used_params = []
        unused_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if param.grad is None:
                    unused_params.append(name)
                else:
                    # ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ ì•„ë‹Œ ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
                    if param.grad.abs().sum().item() > 0:
                        used_params.append(name)
                    else:
                        unused_params.append(name)
        
        print(f"\n===== ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {len(param_status_before)} =====")
        print(f"ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° ìˆ˜: {len(used_params)} ({len(used_params)/len(param_status_before):.2%})")
        print(f"ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° ìˆ˜: {len(unused_params)} ({len(unused_params)/len(param_status_before):.2%})")
        
        # ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° ì¶œë ¥ (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)
        if unused_params:
            print("\në¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° ëª©ë¡:")
            for name in unused_params:
                print(f"- {name}")
        
        # ëª¨ë¸ êµ¬ì¡°ë³„ ì‚¬ìš©/ë¯¸ì‚¬ìš© íŒŒë¼ë¯¸í„° ë¹„ìœ¨ ë¶„ì„
        module_stats = {}
        for name in param_status_before.keys():
            # ëª¨ë“ˆ ì´ë¦„ ì¶”ì¶œ (ì²« ë²ˆì§¸ dotê¹Œì§€)
            module_name = name.split('.')[0] if '.' in name else 'base'
            
            if module_name not in module_stats:
                module_stats[module_name] = {'used': 0, 'unused': 0, 'total': 0}
            
            module_stats[module_name]['total'] += 1
            if name in used_params:
                module_stats[module_name]['used'] += 1
            else:
                module_stats[module_name]['unused'] += 1
        
        print("\nëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ì‚¬ìš© í˜„í™©:")
        for module_name, stats in module_stats.items():
            used_percent = stats['used'] / stats['total'] * 100 if stats['total'] > 0 else 0
            print(f"{module_name}: {stats['used']}/{stats['total']} ì‚¬ìš© ({used_percent:.1f}%)")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.model.zero_grad()
        
        return used_params, unused_params
    
    
    def log_text(self, key: str, value: str, step: int):
        """
        WandBì— í…ìŠ¤íŠ¸ë¥¼ ë¡œê¹…í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜.
        PyTorch Lightningì˜ logger.experimentë¥¼ í†µí•´ WandB APIì— ì ‘ê·¼í•©ë‹ˆë‹¤.
        """
        if self.logger and hasattr(self.logger, "experiment") and isinstance(self.logger.experiment, wandb.sdk.wandb_run.Run):
            # self.logger.experimentëŠ” WandB Run ê°ì²´ì…ë‹ˆë‹¤.
            # wandb.log()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
            self.logger.experiment.log({key: value}, step=step)
        else:
            # WandB ë¡œê±°ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë‹¤ë¥¸ ë¡œê±°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì½˜ì†”ì— ì¶œë ¥
            print(f"Log (Step {step}) - {key}: {value}")